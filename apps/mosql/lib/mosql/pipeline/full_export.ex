defmodule MS.Pipeline.FullExport do
  @moduledoc """
  Documentation for `FullExport`.
  """
  use Broadway

  alias Broadway.Message
  alias MS.Pipeline.FullExportProducer
  alias MS.Mongo

  require Logger

  @doc """
  Kick of the migration process for the given namespace
  """
  def trigger(ns) do
    FullExportProducer.trigger(ns)
  end

  def export_status(ns) do
    FullExportProducer.info_producer_status(ns)
  end

  def start_link(_opts) do
    {:ok, pid} =
      Broadway.start_link(__MODULE__,
        name: __MODULE__,
        producer: [
          module:
            {FullExportProducer,
             %{
               export_triggered: false,
               collections: [],
               exported_collections: [],
               demand_filled: 0,
               pending_demand: 0
             }},
          transformer: {__MODULE__, :transform, []}
        ],
        processors: [
          default: [concurrency: 1, max_demand: 3, min_demand: 1]
        ],
        batchers: [
          default: [concurrency: 1, batch_size: 1]
        ]
      )

    Logger.info("full export pipeline pid #{inspect(pid)}")
    {:ok, pid}
  end

  # Callback method for Broadway.Processor. This is the place to prepare
  # and preload any information that will be used by handle_message/3.
  # For example, if you need to query the database, instead of doing it
  # once per message, you can do it on this callback. The length of the list
  # of messages received by this callback is based on the min_demand/max_demand
  # configuration in the processor. This callback must always return all messages
  # it receives, as handle_message/3 is still called individually for each
  # message afterwards
  @impl true
  def prepare_messages(messages, context) do
    Logger.debug(
      "Handling callback `prepare_messages`. messages: #{inspect(messages)}, context: #{inspect(context)}"
    )

    messages =
      Enum.map(messages, fn message ->
        Logger.info("Fetching documents for the collection #{message.data}")
        cursor = Mongo.find_all(message.data, 3)
        Message.put_data(message, Enum.to_list(cursor))
      end)

    messages
  end

  # Callback method for Broadway.Processor - This is the place to do any kind
  # of processing with the incoming message, e.g., transform the data into
  # another data structure, call specific business logic to do calculations.
  # Basically, any CPU bounded task that runs against a single message should
  # be processed here. The 3 arguments received are:
  #   processor is the key that defined the processor.
  #   message is the Broadway.Message struct to be processed.
  #   context is the user defined data structure passed to start_link/2.
  @impl true
  def handle_message(processor, message, context) do
    IO.puts("=====handler begin=======")

    Logger.debug(
      "processor: #{inspect(processor)}, message: #{inspect(message)}, context: #{inspect(context)}"
    )

    # message |> update_data(&do_calculation_and_returns_the_new_data/1)
    IO.puts("=====handler end=======")
    Message.put_batcher(message, :default)
  end

  @impl true
  def handle_batch(:default, messages, _batch_info, _context) do
    # Send batch of successful messages as ACKs to SQS
    # This tells SQS that this list of messages were successfully processed
    IO.puts("=====batcher begin=======")
    Logger.debug("#{inspect(messages)}")
    IO.puts("=====batcher end=======")
    Process.sleep(1000)
    IO.puts("=====FINAL OUTPUT=======")
    messages
  end

  # Producer transformer to transformer the event data generated by the producer
  # to %Broadway.Message{}
  def transform(event, opts) do
    IO.puts("Transform event: #{inspect(event)}, opts: #{inspect(opts)}")

    %Message{
      data: event,
      acknowledger: {__MODULE__, :ack_id, :ack_data}
    }
  end

  def ack(:ack_id, successful, failed) do
    # Write ack code here
    IO.puts("Ack: #{inspect(successful)} - #{inspect(failed)}")
    :ok
  end
end
